{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_2_NLP.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taylorvroman09/Taylor-Public/blob/main/Project_2_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu0Kz0Nq4ui4"
      },
      "source": [
        "# Project \\#2 Starter Code\n",
        "Your project should address the categories below. \n",
        "\n",
        "## Problem:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "State the problem you are trying to solve with this machine learning experiment. Include a description of the data, and what you're trying to predict. What are the possible uses for this kind of machine learning model?\n",
        "\n",
        "We are trying to determine the sentiment of the movie reviews based on the words in the review itself. The data contains 50,000 reviews with one column, \"review,\" containing the review itself and another column, \"sentiment,\" that describes the review as \"positive\" or \"negative.\""
      ],
      "metadata": {
        "id": "-fJR_8NBxNpZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBZAVjcWhgBV"
      },
      "source": [
        "# Input Pipeline (sklearn):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "12XGv2fIfTSr",
        "outputId": "4f044753-9869-49b9-c90c-3008f703cbbb"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas\n",
        "drive.mount('/content/drive')\n",
        "data = pandas.read_csv('/content/drive/MyDrive/MachineLearning/IMDB_dataset.csv')\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICUC-PvQ5bP2"
      },
      "source": [
        "## Data Exploration:\n",
        "- Number of samples\n",
        "- Number of classes of the target variable\n",
        "- Number of words per sample\n",
        "- Distribution of sample length\n",
        "- Something else: get creative :) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kkCmgdf5ZqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b4c493-a999-4b0c-b8d9-fec1d7f93270"
      },
      "source": [
        "## Use cells here to explore the data:\n",
        "##number of samples\n",
        "print(\"Number of samples: \", len(data))\n",
        "print(\"Number of classes of the target variable: \", len(data.sentiment.unique()))\n",
        "dm = data['review'].str.split().apply(len).value_counts()\n",
        "print(\"Number of words per sample: \", dm.median())\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples:  50000\n",
            "Number of classes of the target variable:  2\n",
            "Number of words per sample:  15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSuguZ6u5lUl"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "I'm providing you with code that cleans the reviews by making it all lowercase letters and removing stop words. The three cells below do this for you. I still want you to explain what you did with the data here. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word embeddings, stop words, vectorization, tokeniztion,"
      ],
      "metadata": {
        "id": "h8eOe3iUV-xw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV4FfvLegTSh",
        "outputId": "ed1585e9-a4f7-4d2f-9ef8-f7b66fb77ec9"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "#only do next line once\n",
        "nltk.download() #in Corpora tab, download stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "#The NLTK downloader will open, you need to select (d) for Download, and then 'stopwords'then (q) to quit"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> stopwords\n",
            "    Downloading package stopwords to /root/nltk_data...\n",
            "      Unzipping corpora/stopwords.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTEr6vRUgOUs"
      },
      "source": [
        "#This is a function that takes in a review, makes sure it is only lower case letters and removes stopwords.\n",
        "#It returns the cleaned review text.\n",
        "def clean_review(review):\n",
        "    #input is a string review\n",
        "    #return is review cleaned of all punctuation, lowercase, and removed nltk stopwords\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\",\" \",review)\n",
        "    lower_case = letters_only.lower()\n",
        "    words = lower_case.split()\n",
        "    for stop_word in stopwords.words(\"english\"):\n",
        "        while stop_word in words:\n",
        "            words.remove(stop_word)\n",
        "    cleaned = \" \".join(words)\n",
        "    return cleaned"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEjHsILZgk9V"
      },
      "source": [
        "#process the data\n",
        "cleaned_text = []\n",
        "for i in range(len(data)):\n",
        "    cleaned_text.append(clean_review(data[\"review\"][i]))  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMg3P0ZNBvGM",
        "outputId": "5dad51cc-5155-4f36-c8c0-754e3322f16b"
      },
      "source": [
        "cleaned_text[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one reviewers mentioned watching oz episode hooked right exactly happened br br first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use word br br called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home many aryans muslims gangstas latinos christians italians irish scuffles death stares dodgy dealings shady agreements never far away br br would say main appeal show due fact goes shows dare forget pretty pictures painted mainstream audiences forget charm forget romance oz mess around first episode ever saw struck nasty surreal say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards sold nickel inmates kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewing thats get touch darker side',\n",
              " 'wonderful little production br br filming technique unassuming old time bbc fashion gives comforting sometimes discomforting sense realism entire piece br br actors extremely well chosen michael sheen got polari voices pat truly see seamless editing guided references williams diary entries well worth watching terrificly written performed piece masterful production one great master comedy life br br realism really comes home little things fantasy guard rather use traditional dream techniques remains solid disappears plays knowledge senses particularly scenes concerning orton halliwell sets particularly flat halliwell murals decorating every surface terribly well done',\n",
              " 'thought wonderful way spend time hot summer weekend sitting air conditioned theater watching light hearted comedy plot simplistic dialogue witty characters likable even well bread suspected serial killer may disappointed realize match point risk addiction thought proof woody allen still fully control style many us grown love br br laughed one woody comedies years dare say decade never impressed scarlet johanson managed tone sexy image jumped right average spirited young woman br br may crown jewel career wittier devil wears prada interesting superman great comedy go see friends',\n",
              " 'basically family little boy jake thinks zombie closet parents fighting time br br movie slower soap opera suddenly jake decides become rambo kill zombie br br ok first going make film must decide thriller drama drama movie watchable parents divorcing arguing like real life jake closet totally ruins film expected see boogeyman similar movie instead watched drama meaningless thriller spots br br well playing parents descent dialogs shots jake ignore',\n",
              " 'petter mattei love time money visually stunning film watch mr mattei offers us vivid portrait human relations movie seems telling us money power success people different situations encounter br br variation arthur schnitzler play theme director transfers action present time new york different characters meet connect one connected one way another next person one seems know previous point contact stylishly film sophisticated luxurious look taken see people live world live habitat br br thing one gets souls picture different stages loneliness one inhabits big city exactly best place human relations find sincere fulfillment one discerns case people encounter br br acting good mr mattei direction steve buscemi rosario dawson carol kane michael imperioli adrian grenier rest talented cast make characters come alive br br wish mr mattei good luck await anxiously next work']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spDgSTvCg9wk"
      },
      "source": [
        "#establish training and testing dataset\n",
        "train_data, test_data, train_sln, test_sln = \\\n",
        "    train_test_split(cleaned_text, data['sentiment'], test_size = 0.2, random_state=0) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-86AHOAkDpge"
      },
      "source": [
        "### Vectorizing the data\n",
        "\n",
        "**CountVectorizer**: Convert a collection of text documents to a matrix of token counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwmex98NDgqJ"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "\n",
        "#Bag of Words with 5000 most common words\n",
        "vectorizer = CountVectorizer(analyzer='word', max_features = 50)\n",
        "#find the right 5000 words\n",
        "vectorizer.fit(train_data)\n",
        "\n",
        "#use the vectorizer to transform review strings into word count vectors \n",
        "train_data_vectors = vectorizer.transform(train_data).toarray()\n",
        "test_data_vectors = vectorizer.transform(test_data).toarray()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Classifer\n",
        "Principal Component Analysis\n",
        "Perceptron\n",
        "MLP"
      ],
      "metadata": {
        "id": "LkuA_BIhXQxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Metrics*\n",
        "\n",
        "- What metrics will you use to evaluate your model? Why are these metrics the best for your model? (Hint, this should be more than 'accuracy').\n",
        "\n",
        "- I will use a Support Vector Classifier, Principal Component Analysis, a Perceptron, and an MLP to determine which approach most effectively evaluates my model. By using the accuracy of *each* "
      ],
      "metadata": {
        "id": "40y1LpezO2gE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSQcsLOEAec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22a0ec97-59a0-45fd-9390-ff399de4ded2"
      },
      "source": [
        "## Now use train_data_vectors and test_data_vectors to train/test/tune your sklearn models.\n",
        "##SVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "clf = SVC(kernel = 'linear', degree = 5)\n",
        "clf.fit(train_data_vectors,train_sln)\n",
        "predictions = clf.predict(test_data_vectors)\n",
        "print(\"accuracy:\", metrics.accuracy_score(test_sln, predictions))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.7047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##look up different things to tune\n",
        "from sklearn.linear_model import Perceptron\n",
        "perc = Perceptron(penalty = 'elasticnet')\n",
        "perc.fit(train_data_vectors,train_sln)\n",
        "p_predictions = perc.predict(test_data_vectors)\n",
        "\n",
        "#output accuracy\n",
        "print(\"Accuracy:\", metrics.accuracy_score(test_sln, p_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_HreBh1ZxPr",
        "outputId": "fc531152-53cb-442f-f9b1-49078256639c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "extractor = PCA(n_components=2, whiten=True)\n",
        "extractor.fit(train_data_vectors)\n",
        "print('this is the variance/importance of each component')\n",
        "print(extractor.explained_variance_ratio_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01c_Cr9LaCgu",
        "outputId": "7c605adb-ae9d-459d-d3bd-538b9e184dfc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is the variance/importance of each component\n",
            "[0.46335543 0.09410852]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "mlp = MLPClassifier(random_state=0,hidden_layer_sizes = (100,), max_iter = 800)\n",
        "mlp.fit(train_data_vectors,train_sln)\n",
        "predictions = mlp.predict(test_data_vectors)\n",
        "\n",
        "print(\"Accuracy: \", metrics.accuracy_score(test_sln,predictions))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqzWayIub8F2",
        "outputId": "02f2a5f3-08af-42b9-e09c-136f31308815"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.6715\n"
          ]
        }
      ]
    }
  ]
}